---
title: "Task 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Task 1:** Build a logistic model to classify the images into malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.

The variable "Diagnosis" is a binary response variable indicating if the image is coming from cancer tissue or benign cases (M = malignant, B = benign). In the following logistic regression model, the "Diagnosis" variable will be coded as 1 for malignant cases and 0 for benign cases.

Given $n$ i.i.d. observations with $p$ predictors, we consider a logistic regression model
\begin{equation}\label{model}
P(Y_i=1\mid \mathbf{x}_i)=\frac{e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}},\; i=1,\ldots,n
\end{equation}
where $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)^\top\in\mathbb{R}^{p+1}$ is the parameter vector, $\mathbf{x}_i=(1,X_{i1},\ldots,X_{ip})^\top$ is the vector of predictors in the $i$-th observation, and $Y_i\in\{0,1\}$ is the binary response in the $i$-th observation.
Let $\mathbf{y}=(Y_1,Y_2,\ldots,Y_n)^\top$ denote the response vector, $\mathbf{X}=(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n)^\top\in\mathbb{R}^{n\times(p+1)}$ denote the design matrix. The observed likelihood of $\{(Y_1,\mathbf{x}_1),(Y_2,\mathbf{x}_2)\ldots,(Y_n,\mathbf{x}_n)\}$ is
$$L(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\prod_{i=1}^n\left[\left(\frac{e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}\right)^{Y_i}\left(\frac{1}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}\right)^{1-Y_i}\right].$$
Maximizing the likelihood is equivalent to maximizing the log-likelihood function:
\begin{equation}\label{func}
f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\sum_{i=1}^n\left[Y_i\mathbf{x}_i^\top\boldsymbol{\beta}-\log\left(1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\right)\right].
\end{equation}
The estimates of model parameters are
$$\widehat{\boldsymbol{\beta}}=\arg\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta};\mathbf{y},\mathbf{X}),$$
and the optimization problem is
\begin{equation}\label{opt}
\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta};\mathbf{y},\mathbf{X}).
\end{equation}
Denote $p_i=P(Y_i=1\mid\mathbf{x}_i)$ as given in (\ref{model}) and $\mathbf{p}=(p_1,p_2,\ldots,p_n)^\top$. The gradient of $f$ is
\begin{align*}
\nabla f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})&=\mathbf{X}^\top(\mathbf{y}-\mathbf{p})\\
&=\sum_{i=1}^n(Y_i-p_i)\mathbf{x}_i\\
&=\begin{pmatrix}
\sum_{i=1}^n(Y_i-p_i)\\ \sum_{i=1}^n(Y_i-p_i)X_{i1}\\ \vdots\\ \sum_{i=1}^n(Y_i-p_i)X_{ip}\end{pmatrix}.
\end{align*}
Denote $w_i=p_i(1-p_i)\in(0,1)$ and $\mathbf{W}=\mathrm{diag}(w_1,\ldots,w_n)$. The Hessian matrix of $f$ is given by
\begin{align*}
\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})&=-\mathbf{X}^\top\mathbf{W}\mathbf{X}\\
&=-\sum_{i=1}^nw_i\mathbf{x}_i\mathbf{x}_i^\top\\
&=-\begin{pmatrix}
\sum_{i=1}^nw_i & \sum_{i=1}^nw_iX_{i1} & \cdots & \sum_{i=1}^nw_iX_{i1} \\ 
\sum_{i=1}^nw_iX_{i1} & \sum_{i=1}^nw_iX_{i1}^2 & \cdots & \sum_{i=1}^nw_iX_{i1}X_{ip} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\sum_{i=1}^nw_iX_{ip} & \sum_{i=1}^nw_iX_{in}X_{i1} & \cdots & \sum_{i=1}^nw_iX_{ip}^2
\end{pmatrix}.
\end{align*}
Next, we show that the Hessian matrix $\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})$ is a negative-definite matrix if $\mathbf{X}$ has full rank.

***Proof.*** For any $(p+1)$-dimensional nonzero vector $\boldsymbol{\alpha}$, given that $\mathbf{X}$ has full rank, $\mathbf{X}\boldsymbol{\alpha}$ is also a nonzero vector. Since $\mathbf{W}$ is positive-definite, we have
\begin{align*}
\boldsymbol{\alpha}^\top\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})\boldsymbol{\alpha}&=\boldsymbol{\alpha}^\top(-\mathbf{X}^\top\mathbf{W}\mathbf{X})\boldsymbol{\alpha}\\
&=-(\mathbf{X}\boldsymbol{\alpha})^\top\mathbf{W}(\mathbf{X}\boldsymbol{\alpha})\\
&<0.
\end{align*}
Thus, $\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})$ is negative-definite. \hfill$\square$

Hence, the optimization problem (\ref{opt}) is a well-defined problem.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
cancer <- read.csv("C:/Users/yujia/Downloads/breast-cancer.csv") %>%
  janitor::clean_names()%>%
  select(-1,-33) %>%
  mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0)) 
#ID labels individual breast tissue images; 

#The second column 'Diagnonsis' identifies if the image is coming from cancer tissue or benign cases (M=malignant, B = benign). There are 357 benign and 212 malignant cases. 

#The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei;
corr = cancer[2:31] %>% 
  cor()
ggcorrplot(corr, type = "upper", tl.cex = 8)
```


```{r logstic full model,echo=FALSE}
library(caret)
library(pROC)
set.seed(1)
trainRows <- createDataPartition(y = cancer$diagnosis, p = 0.8, list = FALSE)
train <- cancer[trainRows, ]
test <-  cancer[-trainRows, ]
glm.fit <- glm(diagnosis ~ ., 
               data = train, 
               subset = trainRows, 
               family = binomial(link = "logit"))
summary(glm.fit)
pred <- predict(glm.fit, newdata = test, type = "response")
y_test <- factor(test$diagnosis)
auc_full <- auc(y_test, pred)
auc_full #0.9641
```


```{r}
#if removing variables collinearity 
cancer1 <- cancer %>%
  select(-area_se, 
         -perimeter_se, 
         -area_worst, 
         -perimeter_mean, 
         -perimeter_worst, 
         -area_mean, 
         -radius_worst, 
         -concave_points_mean, 
         -texture_worst, 
         -compactness_mean, 
         -smoothness_worst)
corr1 = cancer1[2:20] %>% 
  cor()
ggcorrplot(corr1, type = "upper", tl.cex = 8)

set.seed(2)
trainRows1 <- createDataPartition(y = cancer1$diagnosis, 
                                 p = 0.8, list = FALSE)
train1 <- cancer1[trainRows1, ]
test1 <-  cancer1[-trainRows1, ]
glm.fit1 <- glm(diagnosis ~ ., 
               data = train1, 
               subset = trainRows1, 
               family = binomial(link = "logit"))
summary(glm.fit1)
pred1 <- predict(glm.fit1, newdata = test1, type = "response")
y_test1 <- factor(test1$diagnosis)
auc_full1 <- auc(y_test1, pred1)
auc_full1 #0.9962
```



















