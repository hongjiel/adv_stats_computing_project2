---
title: |
  | Breast Cancer Diagnosis
author: |
  | Hongjie Liu, Xicheng Xie, Jiajun Tao, Shaohan Chen, Yujia Li
date: "2023-02-27"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
   - \usepackage{algorithm}
   - \usepackage{algpseudocode}
output:
  beamer_presentation:
    colortheme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret)
library(sigmoid) 
library(qgam) 
library(pROC)

# magic that automatically adjusts the font size
def.chunk.hook = knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x = def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```


## Outline

- Background

- Task Introduction

- Task 1

- Task 2

- Task 3

- Task 4

- Discussions

- Limitations and Future Work

- Reference

- Q&A


## Background

```{r include=FALSE}
# The data
```

**Breast Cancer Diagnosis:**\par
- In this project we study the breast cancer diagnosis problem

- The goal of the exercise is to build a predictive model based on logistic regression to facilitate cancer diagnosis

- We move towards the goal with the steps of task 1, 2, 3 and 4.

## Background 
**Data Source:**\par
- The data is the breast cancer medical data retrieved from "breast-cancer.csv", which has 569 rows and 32 columns

- The first column `ID` labels individual breast tissue images; The second column `Diagnosis` identifies if the image is coming from cancer tissue or benign
cases (M = malignant, B = benign). There are 357 benign and 212 malignant cases

- The other 30 columns correspond to mean, standard deviation and the largest values (points on the tails) of the distributions of the following 10 features computed for the cellnuclei


## Task Introduction

- Task 1: Build a logistic model to classify the images into malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.
 
- Task 2: Develop a Newton-Raphson algorithm to estimate your model.

- Task 3: Build a logistic-LASSO model to select features, and implement a path-wise coordinate-wise optimization algorithm to obtain a path of solutions with a sequence of descending $\lambda$’s.

- Task 4: Use 5-fold cross-validation to select the best $\lambda$. Compare the prediction performance between the ’optimal’ model and ’full’ model


## Task 1 - Objective
**Objective:**

Build a logistic model to classify the images into malignant/benign, and write down your likelihood function, its gradient and Hessian matrix.


## Task 1 - Build a logistic model
Define the "Diagnosis" variable will be coded as 1 for malignant cases and 0 for benign cases.\par

Given $n$ i.i.d. observations with $p$ predictors, we consider a logistic regression model
\begin{equation}\label{model}
P(Y_i=1\mid \mathbf{x}_i)=\frac{e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}},\; i=1,\ldots,n
\end{equation}

where $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)^\top\in\mathbb{R}^{p+1}$ is the parameter vector, $\mathbf{x}_i=(1,X_{i1},\ldots,X_{ip})^\top$ is the vector of predictors in the $i$-th observation, and $Y_i\in\{0,1\}$ is the binary response in the $i$-th observation.

## Task 1 - Build a logistic model
Let $\mathbf{y}=(Y_1,Y_2,\ldots,Y_n)^\top$ denote the response vector, and $\mathbf{X}=(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n)^\top\in\mathbb{R}^{n\times(p+1)}$ denote the design matrix.

The observed likelihood of $\{(Y_1,\mathbf{x}_1),(Y_2,\mathbf{x}_2)\ldots,(Y_n,\mathbf{x}_n)\}$ is
$$L(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\prod_{i=1}^n\left[\left(\frac{e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}\right)^{Y_i}\left(\frac{1}{1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}}\right)^{1-Y_i}\right]$$


## Task 1 - Build a logistic model
Maximizing the likelihood is equivalent to maximizing the log-likelihood function:
\begin{equation}\label{func}
f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\sum_{i=1}^n\left[Y_i\mathbf{x}_i^\top\boldsymbol{\beta}-\log\left(1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\right)\right].
\end{equation}
The estimates of model parameters are
$$\widehat{\boldsymbol{\beta}}=\arg\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta};\mathbf{y},\mathbf{X}),$$
and the optimization problem is
\begin{equation}\label{opt}
\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta};\mathbf{y},\mathbf{X}).
\end{equation}


## Task 1 - Build a logistic model
Denote $p_i=P(Y_i=1\mid\mathbf{x}_i)$ as given in (\ref{model}) and $\mathbf{p}=(p_1,p_2,\ldots,p_n)^\top$. The gradient of $f$ is:
\begin{align*}
\nabla f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})&=\mathbf{X}^\top(\mathbf{y}-\mathbf{p})\\
&=\sum_{i=1}^n(Y_i-p_i)\mathbf{x}_i\\
&=\begin{pmatrix}
\sum_{i=1}^n(Y_i-p_i)\\ \sum_{i=1}^n(Y_i-p_i)X_{i1}\\ \vdots\\ \sum_{i=1}^n(Y_i-p_i)X_{ip}\end{pmatrix}
\end{align*}


## Task 1 - Build a logistic model
Denote $w_i=p_i(1-p_i)\in(0,1)$ and $\mathbf{W}=\mathrm{diag}(w_1,\ldots,w_n)$. The Hessian matrix of $f$ is given by
\begin{align*}
\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})&=-\mathbf{X}^\top\mathbf{W}\mathbf{X}\\
&=-\sum_{i=1}^nw_i\mathbf{x}_i\mathbf{x}_i^\top\\
&=-\begin{pmatrix}
\sum_{i=1}^nw_i & \sum_{i=1}^nw_iX_{i1} & \cdots & \sum_{i=1}^nw_iX_{i1} \\ 
\sum_{i=1}^nw_iX_{i1} & \sum_{i=1}^nw_iX_{i1}^2 & \cdots & \sum_{i=1}^nw_iX_{i1}X_{ip} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
\sum_{i=1}^nw_iX_{ip} & \sum_{i=1}^nw_iX_{in}X_{i1} & \cdots & \sum_{i=1}^nw_iX_{ip}^2
\end{pmatrix}
\end{align*}

## Task 1 - Build a logistic model
Next, we show that the Hessian matrix $\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})$ is a negative-definite matrix if $\mathbf{X}$ has full rank.

***Proof.*** For any $(p+1)$-dimensional nonzero vector $\boldsymbol{\alpha}$, given that $\mathbf{X}$ has full rank, $\mathbf{X}\boldsymbol{\alpha}$ is also a nonzero vector. Since $\mathbf{W}$ is positive-definite, we have
\begin{align*}
\boldsymbol{\alpha}^\top\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})\boldsymbol{\alpha}&=\boldsymbol{\alpha}^\top(-\mathbf{X}^\top\mathbf{W}\mathbf{X})\boldsymbol{\alpha}\\
&=-(\mathbf{X}\boldsymbol{\alpha})^\top\mathbf{W}(\mathbf{X}\boldsymbol{\alpha})\\
&<0.
\end{align*}
Thus, $\nabla^2 f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})$ is negative-definite. \hfill$\square$

Hence, the optimization problem (\ref{opt}) is a well-defined problem.


## Task 1 - Logistic model R code
```{r eval = FALSE, size = "tiny"}
cancer <- read.csv("breast-cancer.csv") %>%
janitor::clean_names()%>%
select(-1,-33) %>%
mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0))
cor()
ggcorrplot(corr, type = "upper", tl.cex = 8)

set.seed(1)
trainRows <- createDataPartition(y = cancer$diagnosis, p = 0.8, list = FALSE)
train <- cancer[trainRows, ]
test <-  cancer[-trainRows, ]
glm.fit <- glm(diagnosis ~ ., 
               data = train, 
               subset = trainRows, 
               family = binomial(link = "logit"))
summary(glm.fit)
pred <- predict(glm.fit, newdata = test, type = "response")
y_test <- factor(test$diagnosis)
auc_full <- auc(y_test, pred)
auc_full 
```

- Final AUC of full model reaches 0.9641. 

- And if we remove correlated variables, the AUC is uplifted to 0.9962.




## Task 2 - Objective
**Objective:**\par
Develop a Newton-Raphson algorithm to estimate your model

- Recall The target function $f$ given in task 1 is:
$$f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\sum_{i=1}^n\left[Y_i\mathbf{x}_i^\top\boldsymbol{\beta}-\log\left(1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\right)\right].$$
- We develop a modified Newton-Raphson algorithm including a step-halving step to maximize the target function.


## Task 2 - Newton-Raphson algorithm design
\tiny
\begin{algorithm}[H]
	\caption{Newton-Raphson algorithm}
	\begin{algorithmic}
	  \Require $f(\boldsymbol{\beta})$ - target function as given in (\ref{func}); $\boldsymbol{\beta}_0$ - starting value
	  \Ensure $\widehat{\boldsymbol{\beta}}$ such that $\widehat{\boldsymbol{\beta}} \approx \arg\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta})$
	  \State $i\leftarrow 0$, where $i$ is the current number of iterations
	  \State $f(\boldsymbol{\beta}_{-1})\leftarrow -\infty$
	  \While {convergence criterion is not met}
	    \State $i \leftarrow i+1$
	    \State $\mathbf{d}_i\leftarrow-[\nabla^2f(\boldsymbol{\beta}_{i-1})]^{-1}\nabla f(\boldsymbol{\beta}_{i-1})$, where $\mathbf{d}_i$ is the direction in the $i$-th iteration
	    \State $\lambda_i \leftarrow 1$, where $\lambda_i$ is the multiplier in the $i$-th iteration
	    \State $\boldsymbol{\beta}_i \leftarrow \boldsymbol{\beta}_{i-1} + \lambda_i\mathbf{d}_i$
	    \While {$f(\boldsymbol{\beta}_i)\le f(\boldsymbol{\beta}_{i-1})$}
	      \State $\lambda_i \leftarrow \lambda_i/2$
	      \State $\boldsymbol{\beta}_i \leftarrow \boldsymbol{\beta}_{i-1} + \lambda_i\mathbf{d}_i$
	    \EndWhile
	  \EndWhile
	  \State $\widehat{\boldsymbol{\beta}}\leftarrow \boldsymbol{\beta}_i$
	\end{algorithmic}
\end{algorithm}


## Task 2 - Newton-Raphson algorithm R code
```{r eval=FALSE, size = "tiny"}
NewtonRaphson <- function(dat, func, start, tol = 1e-8, maxiter = 200) {
  i <- 0
  cur <- start
  stuff <- func(dat, cur)
  res <- c(0, stuff$f, cur)
  prevf <- -Inf
  X <- cbind(rep(1, nrow(dat)), as.matrix(dat[, -1]))
  y <- dat[, 1]
  warned <- 0
  while (abs(stuff$f - prevf) > tol && i < maxiter) {
    i <- i + 1
    prevf <- stuff$f
    prev <- cur
    d <- -solve(stuff$Hess) %*% stuff$grad
    cur <- prev + d
    lambda <- 1
    maxhalv <- 0
    while (func(dat, cur)$f < prevf && maxhalv < 50) {
      maxhalv <- maxhalv + 1
      lambda <- lambda / 2
      cur <- prev + lambda * d
    }
    stuff <- func(dat, cur)
    res <- rbind(res, c(i, stuff$f, cur))
    y_hat <- ifelse(X %*% cur > 0, 1, 0)
    if (warned == 0 && sum(y - y_hat) == 0) {
      warning("Complete separation occurs. Algorithm does not converge.")
      warned <- 1
    }
  }
  colnames(res) <- c("iter", "target_function", "(Intercept)", names(dat)[-1])
  return(res)
}
```


## Task 2 - Complete separtion
- Sometimes our algorithm does not converge because of the complete separation.

- A complete separation in a logistic regression, sometimes also referred as perfect prediction, occurs whenever there exists some vector of coefficients $\boldsymbol{\beta}$ such that $Y_i = 1$ whenever $\mathrm{x}_i^\top\boldsymbol{\beta} > 0$ and $Y_i = 0$ whenever $\mathrm{x}_i^\top\boldsymbol{\beta} \le 0$. 

- Complete separation occur when a linear function of predictors can perfectly classify the response.


## Task 2 - Complete separation
\begin{figure}[H] 
\includegraphics[width=0.8\textwidth]{images/complete_separation.jpeg} 
\end{figure}


## Task 2 - Complete separation
- We have proved that: when there exists a vector of coefficients $\hat{\boldsymbol{\beta}}$ such that $Y_i = 1$ whenever $\mathrm{x}_i^\top\hat{\boldsymbol{\beta}} > 0$ and $Y_i = 0$ whenever $\mathrm{x}_i^\top\hat{\boldsymbol{\beta}} \le 0$, there does not exist $\boldsymbol{\beta}^*\in\mathbb{R}^{(p+1)}$ such that $\boldsymbol{\beta}^* = \arg\max_{\boldsymbol{\beta}}f(\boldsymbol{\beta})$, where $f$ is given in (\ref{func}). Thus our algorithm does not converge. (proof is attached in report appendix) \par

- If there is no complete separation, the parameters output by `glm` function and our algorithm are demonstrated to be the same

- In practice, if complete separation occurs, we randomly pick the parameters which satisfy complete separation as our full model

## Task 2 - Comparison
Comparison of using `glm` function and our algorithm (part of)
```{r echo = FALSE, warning = FALSE, size = "tiny"}
load("data/task2.RData")
c_t = tibble(
  predictor = c("(Intercept)", names(Training)[-1]),
  ours = res[nrow(res), -c(1, 2)],
  glm = glm(diagnosis ~ ., family = binomial(link = "logit"), data = Training)$coefficients
) 
c_t[1:20, ]%>% knitr::kable()
```




## Task 3 - Objective
**Objective:**\par
Build a logistic-LASSO model to select features by implementing a path-wise coordinate-wise optimization algorithm

- Recall Log-likelihood $f$ in task 1:
\begin{equation}\label{func}
f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\sum_{i=1}^n\left[Y_i\mathbf{x}_i^\top\boldsymbol{\beta}-\log\left(1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\right)\right].
\end{equation}

- LASSO estimates the logistic model parameters $\boldsymbol{\beta}$ by optimizing a penalized loss function:
\begin{equation}\label{opt.lasso}
\min_{\boldsymbol{\beta}}\; -\frac{1}{n}f(\boldsymbol{\beta})+\lambda\sum_{k=1}^{p}|\beta_k|.
\end{equation}
where $\lambda\ge 0$ is the tuning parameter. Note that the intercept is not penalized and all predictors are standardized.

## Task 3 - Algorithm of Logistic-LASSO Model
**Algorithm Structure:**\par

- OUTER LOOP: Decrement $\lambda$.

- MIDDLE LOOP: Update $\tilde{w}_i$, $\tilde{p}_i$, and thus the quadratic approximation $\ell$ using the current
parameters $\tilde{\boldsymbol{\beta}}$.

- INNER LOOP: Run the coordinate descent algorithm on the penalized weighted-least-squares problem.

## Task 3 - OUTER LOOP
**OUTER LOOP: **\par
Compute the solutions of the optimization problem (\ref{opt.lasso}) for a decreasing sequence of values for $\lambda$: $\{\lambda_1,\ldots,\lambda_m\}$, starting at the smallest value $\lambda_1 = \lambda_{max}$
\begin{equation}\label{maxlambda}
\lambda_{max} = \frac{1}{n}\max_j\left|\left \langle \mathbf{x}_{\cdot j}, \mathbf{y} \right \rangle\right|,
\end{equation}
where $\mathbf{x}_{\cdot j}$ is the $j$-th column of the design matrix $\mathbf{X}$, for $j=1,\ldots,p$. \par
For tuning parameter value $\lambda_{k+1}$, we initialize coordinate descent algorithm at the computed solution for $\lambda_k$ (warm start).

```{r include=FALSE}
# Task 3
```


## Task 4

```{r include=FALSE}
# Task 4
```


## Discussions

- There is much freedom when designing the simulations.

- In our algorithm, we have 5 parameters. n, p, ratio, c, corr

- More parameters can be adjusted.

## Limitations and Future Work

- Limitation: We reproduced high-dimensional scenarios, but we still don't know the solution.

- Future Work: We may adjust other parameters to investigate further.

## Reference

1. Li Y, Hong HG, Ahmed SE, Li Y. Weak signals in high‐dimensional regression: Detection, estimation and prediction. Appl Stochastic Models Bus Ind. 2018;1–16. https://doi.org/10.1002/asmb.2340


## Q&A

- Thanks for listening!

- Any questions?