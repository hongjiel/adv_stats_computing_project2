---
title: "Task 2"
output: pdf_document
header-includes:
  - \usepackage{algorithm}
  - \usepackage{algpseudocode}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(sigmoid) # function `sigmoid` to compute exp(x)/(1 + exp(x))
library(qgam) # function `log1pexp` to compute log(1 + exp(x)))
library(pROC)
```

**Task 2:** Develop a Newton-Raphson algorithm to estimate your model.

The target function $f$ given in task 1:
\begin{equation}\label{func}
f(\boldsymbol{\beta};\mathbf{y},\mathbf{X})=\sum_{i=1}^n\left[Y_i\mathbf{x}_i^\top\boldsymbol{\beta}-\log\left(1+e^{\mathbf{x}_i^\top\boldsymbol{\beta}}\right)\right].
\end{equation}

We develop a modified Newton-Raphson algorithm including a step-halving step. *(we probably don't need to ensure that the direction of the step is an ascent direction, since in this example Hessian is always negative-definite. but Hessian could be computationally singular when the starting points are bad)*

\begin{algorithm}
	\caption{Newton-Raphson algorithm including a step-halving step}
	\begin{algorithmic}[1]
	  \Require $f(\boldsymbol{\beta})$ - target function as given in (\ref{func}); $\boldsymbol{\beta}_0$ - starting value
	  \Ensure $\widehat{\boldsymbol{\beta}}$ such that $\widehat{\boldsymbol{\beta}} \approx \arg\max_{\boldsymbol{\beta}}\; f(\boldsymbol{\beta})$
	  \State $i\leftarrow 0$, where $i$ is the current number of iterations
	  \State $f(\boldsymbol{\beta}_{-1})\leftarrow -\infty$
	  \While {convergence criterion is not met}
	    \State $i \leftarrow i+1$
	    \State $\mathbf{d}_i\leftarrow-[\nabla^2f(\boldsymbol{\beta}_{i-1})]^{-1}\nabla f(\boldsymbol{\beta}_{i-1})$, where $\mathbf{d}_i$ is the direction in the $i$-th iteration
	    \State $\lambda_i \leftarrow 1$, where $\lambda_i$ is the multiplier in the $i$-th iteration
	    \State $\boldsymbol{\beta}_i \leftarrow \boldsymbol{\beta}_{i-1} + \lambda_i\mathbf{d}_i$
	    \While {$f(\boldsymbol{\beta}_i)\le f(\boldsymbol{\beta}_{i-1})$}
	      \State $\lambda_i \leftarrow \lambda_i/2$
	      \State $\boldsymbol{\beta}_i \leftarrow \boldsymbol{\beta}_{i-1} + \lambda_i\mathbf{d}_i$
	    \EndWhile
	  \EndWhile
	  \State $\widehat{\boldsymbol{\beta}}\leftarrow \boldsymbol{\beta}_i$
	\end{algorithmic}
\end{algorithm}

We write an **R**-function `NewtonRaphson` to implement the algorithm. 

```{r}
NewtonRaphson <- function(dat, func, start, tol = 1e-8, maxiter = 200) {
  i <- 0
  cur <- start
  stuff <- func(dat, cur)
  res <- c(0, stuff$f, cur)
  prevf <- -Inf
  X <- cbind(rep(1, nrow(dat)), as.matrix(dat[, -1]))
  y <- dat[, 1]
  warned <- 0
  while (abs(stuff$f - prevf) > tol && i < maxiter) {
    i <- i + 1
    prevf <- stuff$f
    prev <- cur
    d <- -solve(stuff$Hess) %*% stuff$grad
    cur <- prev + d
    lambda <- 1
    maxhalv <- 0
    while (func(dat, cur)$f < prevf && maxhalv < 50) {
      maxhalv <- maxhalv + 1
      lambda <- lambda / 2
      cur <- prev + lambda * d
    }
    stuff <- func(dat, cur)
    res <- rbind(res, c(i, stuff$f, cur))
    y_hat <- ifelse(X %*% cur > 0, 1, 0)
    if (warned == 0 && sum(y - y_hat) == 0) {
      warning("Complete separation occurs. Algorithm does not converge.")
      warned <- 1
    }
  }
  colnames(res) <- c("iter", "target_function", "(Intercept)", names(dat)[-1])
  return(res)
}
```

Data preprocessing and data partition.

```{r}
bc_df <- read.csv("breast-cancer.csv")[-c(1, 33)] %>% # remove variable ID and an NA column
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0)) # code malignant cases as 1
bc_df[, -1] <- scale(bc_df[, -1]) # predictors are standardized for the logistic-LASSO model in task 3

set.seed(1)
indexTrain <- createDataPartition(y = bc_df$diagnosis, p = 0.8, list = FALSE)
Training <- bc_df[indexTrain, ]
Test <- bc_df[-indexTrain, ]

glm(diagnosis ~ ., family = binomial(link = "logit"), data = Training)

logisticstuff <- function(dat, betavec) {
  dat <- as.matrix(dat)
  n <- nrow(dat)
  p <- ncol(dat) - 1
  X <- cbind(rep(1, n), dat[, -1]) # design matrix
  y <- dat[, 1] # response vector
  u <- X %*% betavec # x_i^T beta, i=1,...,n
  f <- sum(y * u - log1pexp(u)) # function `log1pexp` to compute log(1 + exp(x)))
  p_vec <- sigmoid(u) # function `sigmoid` to compute exp(x)/(1 + exp(x))
  grad <- t(X) %*% (y - p_vec)
  Hess <- -t(X) %*% diag(c(p_vec * (1 - p_vec))) %*% X
  return(list(f = f, grad = grad, Hess = Hess))
}
```

We fit a logistic regression model on the training data using our `NewtonRaphson` function.

```{r}
res <- NewtonRaphson(dat = Training, func = logisticstuff, start = rep(0, ncol(Training)))
tail(res)
```

Our function also does not converge, because a complete separation occurs. A complete separation in a logistic regression, sometimes also referred as perfect prediction, which occurs whenever there exists some vector of coefficients $\boldsymbol{\beta}$ such that $Y_i = 1$ whenever $\mathbf{x}_i^\top\boldsymbol{\beta} > 0$ and $Y_i = 0$ whenever $\mathbf{x}_i^\top\boldsymbol{\beta} \le 0$. In other words, complete separation occurs whenever a linear function of predictors can generate perfect predictions of response.

```{r}
X <- cbind(rep(1, nrow(Training)), model.matrix(diagnosis ~ ., Training)[, -1])
y <- Training$diagnosis
coef_newton <- res[nrow(res), -c(1, 2)]
y_hat <- ifelse(X %*% coef_newton > 0, 1, 0) # predictions
sum(y - y_hat) # complete separation
```


We can prove that: when there exists a vector of coefficients $\hat{\boldsymbol{\beta}}$ such that $Y_i = 1$ whenever $\mathbf{x}_i^\top\hat{\boldsymbol{\beta}} > 0$ and $Y_i = 0$ whenever $\mathbf{x}_i^\top\hat{\boldsymbol{\beta}} \le 0$, there does not exist $\boldsymbol{\beta}^*\in\mathbb{R}^{(p+1)}$ such that $\boldsymbol{\beta}^* = \arg\max_{\boldsymbol{\beta}}f(\boldsymbol{\beta})$, where $f$ is given in (\ref{func}). Thus our algorithm does not converge.

***Proof.***

Assume such $\boldsymbol{\beta}^*$ exists, then $\forall \boldsymbol{\beta}\in\mathbb{R}^{p+1}$, we have $f(\boldsymbol{\beta})\le f(\boldsymbol{\beta}^*)$.

First, we prove that: there exists a vector of coefficients $\tilde{\boldsymbol{\beta}}$ such that $Y_i = 1$ whenever $\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}} > 0$ and $Y_i = 0$ whenever $\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}} < 0$.

Let $A_1=\{i: Y_i = 1\}= \{i:\mathbf{x}_i^\top\hat{\boldsymbol{\beta}}> 0\}$ and $A_0=\{i: Y_i = 0\}=\{i:\mathbf{x}_i^\top\hat{\boldsymbol{\beta}}\le0\}$. Then we have $$\epsilon:=\min_{i\in A_1}(\mathbf{x}_i^\top\hat{\boldsymbol{\beta}}) > 0.$$
Let $\tilde{\boldsymbol{\beta}}=\hat{\boldsymbol{\beta}}-(\epsilon/2,0,\ldots,0)^\top$. Given that $X_{i0}=1$ for all $i$, we have 
$$\begin{aligned}
\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}=\mathbf{x}_i^\top\hat{\boldsymbol{\beta}}-\epsilon/2\cdot1\ge \epsilon-\epsilon/2=\epsilon/2>0,\quad \forall i\in A_1\\
\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}=\mathbf{x}_i^\top\hat{\boldsymbol{\beta}}-\epsilon/2\cdot1\le 0-\epsilon/2=-\epsilon/2<0,\quad \forall i\in A_0\\
\end{aligned}$$
Thus we have $A_1=\{i: Y_i = 1\}= \{i:\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}> 0\}$ and $A_0=\{i: Y_i = 0\}=\{i:\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}< 0\}$.

Next, we prove that 
\begin{equation}\label{loglik}
\lim_{k\rightarrow\infty}f(k\tilde{\boldsymbol{\beta}})=0.
\end{equation}
$\forall k>0$, we have $A_1= \{i:\mathbf{x}_i^\top(k\tilde{\boldsymbol{\beta}})> 0\}$ and $A_0==\{i:\mathbf{x}_i^\top(k\tilde{\boldsymbol{\beta}})< 0\}$.

Thus,
$$\begin{aligned}
\lim_{k\rightarrow\infty}f(k\tilde{\boldsymbol{\beta}})&=\lim_{k\rightarrow\infty}\sum_{i\in A_1}\left[Y_i\mathbf{x}_i^\top(k\tilde{\boldsymbol{\beta})}-\log\left(1+e^{\mathbf{x}_i^\top(k\tilde{\boldsymbol{\beta}})}\right)\right]+\lim_{k\rightarrow\infty}\sum_{i\in A_0}\left[Y_i\mathbf{x}_i^\top(k\tilde{\boldsymbol{\beta}})-\log\left(1+e^{\mathbf{x}_i^\top (k\tilde{\boldsymbol{\beta}})}\right)\right]\\
&=\sum_{i\in A_1}\lim_{k\rightarrow\infty}\left[k\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}-\log\left(1+e^{k\mathbf{x}_i^\top\tilde{\boldsymbol{\beta}}}\right)\right]+\sum_{i\in A_0}\lim_{k\rightarrow\infty}\left[-\log\left(1+e^{k\mathbf{x}_i^\top \tilde{\boldsymbol{\beta}}}\right)\right]\\
&=\sum_{i\in A_1}\lim_{z\rightarrow\infty}\left[z-\log\left(1+e^z\right)\right]+\sum_{i\in A_0}\left(-\log1\right)\\
&=0+0=0.
\end{aligned}$$

Last, we prove that: there exists $\boldsymbol{\beta}\in\mathbb{R}^{p+1}$ such that $f(\boldsymbol{\beta})>f(\boldsymbol{\beta}^*)$, which is contradictory to the statement that $\forall \boldsymbol{\beta}\in\mathbb{R}^{p+1}$, $f(\boldsymbol{\beta})\le f(\boldsymbol{\beta}^*)$.

Note that $f(\boldsymbol{\beta})<0$ holds for any $\boldsymbol{\beta}\in\mathbb{R}$, then we have $f(\boldsymbol{\beta}^*)<0$.

Given that $f(\boldsymbol{\beta}^*)<0$ and (\ref{loglik}) holds, there exists $N\in \mathbb{R}$ such that $\forall k>N$, $f(k\tilde{\boldsymbol{\beta}})>f(\boldsymbol{\beta}^*)$.

Thus our assumption must be false. \hfill$\square$


We compare the results of using the `glm` function and our `NewtonRaphson` function. (meaningless, since both do not converge)

```{r warning=FALSE}
task2_compare<-
  tibble(
  predictor = c("(Intercept)", names(Training)[-1]),
  ours = res[nrow(res), -c(1, 2)],
  glm = glm(diagnosis ~ ., family = binomial(link = "logit"), data = Training)$coefficients
)
write_csv(task2_compare,"data/task2_compare.csv")
task2_compare%>% 
  knitr::kable()
```

**We probably won't conduct resampling. Ignore the below.**

Resampling on training data: *Does the following resampling method work?*

```{r eval=FALSE}
?caret::resamples
```

Hothorn et al. The design and analysis of benchmark experiments. Journal of Computational and Graphical Statistics (2005) vol. 14 (3) pp. 675-699

https://ro.uow.edu.au/cgi/viewcontent.cgi?article=3494&context=commpapers

RW-OOB

```{r warning=FALSE, message=FALSE}
B = 100 # number of bootstrap samples
set.seed(1)
auc.logit <- rep(NA, B)
for (i in 1:B) {
  index_bs <- sample(nrow(Training), replace = TRUE)
  sample <- Training[index_bs, ]
  out <- Training[-index_bs, ]
  res <- NewtonRaphson(dat = sample, func = logisticstuff, start = rep(0, ncol(sample)))
  betavec <- res[nrow(res), 3:ncol(res)]
  X <- cbind(rep(1, nrow(out)), model.matrix(diagnosis ~ ., out)[, -1])
  y <- out$diagnosis
  u <- X %*% betavec
  phat <- sigmoid(u)[, 1]
  roc <- roc(response = y, predictor = phat)
  auc <- roc$auc[1]
  auc.logit[i] <- auc
}
summary(auc.logit)
boxplot(auc.logit)
```
